{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Models for O'neil Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "unprocessed = pd.read_excel(\"./O'neil files/data_combination_response.xls\")\n",
    "unprocessed['ic50'] = unprocessed['X/X0'].apply(lambda x: 1 if x>=.45 and x<=0.55 else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./O'neil files/ComboFM Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    # Combine all features excluding drug concentration\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"train_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y['X/X0']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    y['drugA Conc (µM)'] = np.log2(y['drugA Conc (µM)'])\n",
    "    y['drugB Conc (µM)'] = np.log2(y['drugB Conc (µM)'])\n",
    "    X.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = ElasticNet()\n",
    "    model.fit(X, y)\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Validation\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__one-hot_encoding.csv\", \"validation_data_drug2__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__estate_fingerprints.csv\", \"validation_data_drug2__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"test_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y_val['X/X0']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    y_test = y_test[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "    y_test['drugA Conc (µM)'] = np.log2(y_test['drugA Conc (µM)'])\n",
    "    y_test['drugB Conc (µM)'] = np.log2(y_test['drugB Conc (µM)'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./O'neil files/ComboFM Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    # Combine all features excluding drug concentration\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"train_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y['X/X0']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    y['drugA Conc (µM)'] = np.log2(y['drugA Conc (µM)'])\n",
    "    y['drugB Conc (µM)'] = np.log2(y['drugB Conc (µM)'])\n",
    "    X.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X, y)\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Validation\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__one-hot_encoding.csv\", \"validation_data_drug2__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__estate_fingerprints.csv\", \"validation_data_drug2__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"test_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y_val['X/X0']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    y_test = y_test[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "    y_test['drugA Conc (µM)'] = np.log2(y_test['drugA Conc (µM)'])\n",
    "    y_test['drugB Conc (µM)'] = np.log2(y_test['drugB Conc (µM)'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "\n",
    "# Define a simple MLP with 4 layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./O'neil files/ComboFM Real Data Fold{fold + 1}/\"\n",
    "    \n",
    "    features_tensor = pd.DataFrame()\n",
    "    for fname in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "    \n",
    "    # Combine auxiliary features\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for fname in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "    \n",
    "    # Read training labels and add 'X/X0' as a feature\n",
    "    y_train = pd.read_csv(data_dir + f\"train_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y_train['X/X0']\n",
    "    \n",
    "    # Combine all features and remove duplicate columns\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    \n",
    "    # Merge drug concentration labels with features\n",
    "    y = y_train[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    \n",
    "    # Log-transform the labels\n",
    "    y['drugA Conc (µM)'] = np.log2(y['drugA Conc (µM)'])\n",
    "    y['drugB Conc (µM)'] = np.log2(y['drugB Conc (µM)'])\n",
    "    X.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Convert training data to torch tensors\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "   \n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = MLP(input_dim=input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Free training memory\n",
    "    del X, y, X_tensor, y_tensor\n",
    "    gc.collect()\n",
    "    \n",
    "    features_tensor = pd.DataFrame()\n",
    "    for fname in [\"validation_data_drug1__one-hot_encoding.csv\", \n",
    "                  \"validation_data_drug2__one-hot_encoding.csv\", \n",
    "                  \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "    \n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for fname in [\"validation_data_drug1__estate_fingerprints.csv\", \n",
    "                  \"validation_data_drug2__estate_fingerprints.csv\", \n",
    "                  \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "    \n",
    "    # Read validation labels and add 'X/X0' as a feature\n",
    "    y_val = pd.read_csv(data_dir + f\"test_fold_{fold}.csv\")\n",
    "    features_tensor['X/X0'] = y_val['X/X0']\n",
    "    \n",
    "    # Combine features and remove duplicate columns\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "    # Merge drug concentration labels with validation features\n",
    "    y_test = y_val[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"drugA Conc (µM)\", \"drugB Conc (µM)\"]]\n",
    "    X_test.drop([\"drugA Conc (µM)\", \"drugB Conc (µM)\"], axis=1, inplace=True)\n",
    "    \n",
    "    y_test['drugA Conc (µM)'] = np.log2(y_test['drugA Conc (µM)'])\n",
    "    y_test['drugB Conc (µM)'] = np.log2(y_test['drugB Conc (µM)'])\n",
    "    \n",
    "    # Convert validation data to torch tensors\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy arrays for evaluation\n",
    "    pred_np = predictions.numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_test_np, pred_np)\n",
    "    mae = np.mean(np.abs(y_test_np - pred_np))\n",
    "    print(f\"Fold {fold} - Validation MSE: {mse:.4f}\")\n",
    "    print(f\"Fold {fold} - Validation MAE: {mae:.4f}\")\n",
    "    \n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in predictions:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n",
    "\n",
    "    # Free validation memory and model for the next fold\n",
    "    del X_test, y_test, X_test_tensor, y_test_tensor, model\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Models for NCI Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./NCI files/NCI Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    # Combine all features excluding drug concentration\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"NCI_train_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y['PercentageGrowth']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"Conc1\", \"Conc2\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"Conc1\", \"Conc2\"]]\n",
    "    y['Conc1'] = np.log2(y['Conc1'])\n",
    "    y['Conc2'] = np.log2(y['Conc2'])\n",
    "    X.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = ElasticNet()\n",
    "    model.fit(X, y)\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Validation\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__one-hot_encoding.csv\", \"validation_data_drug2__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__estate_fingerprints.csv\", \"validation_data_drug2__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"NCI_test_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y_val['PercentageGrowth']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"Conc1\", \"Conc2\"]]\n",
    "    y_test = y_test[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "    y_test['Conc1'] = np.log2(y_test['Conc1'])\n",
    "    y_test['Conc2'] = np.log2(y_test['Conc2'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./NCI files/NCI Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    # Combine all features excluding drug concentration\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"NCI_train_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y['PercentageGrowth']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"Conc1\", \"Conc2\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"Conc1\", \"Conc2\"]]\n",
    "    y['Conc1'] = np.log2(y['Conc1'])\n",
    "    y['Conc2'] = np.log2(y['Conc2'])\n",
    "    X.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = RandomForestRegressor(\n",
    "    n_estimators=50,         # Reduce the number of trees (default is 100)\n",
    "    max_depth=10,            # Limit the depth of trees (default is None)\n",
    "    min_samples_split=10,     # Increase minimum samples per split (default is 2)\n",
    "    min_samples_leaf=4,       # Increase minimum samples per leaf (default is 1)\n",
    "    max_features='sqrt',      # Use a subset of features (default is 'auto')\n",
    "    n_jobs=-1,                # Use all CPU cores\n",
    "    random_state=42,          # Ensure reproducibility\n",
    "    verbose=0                 # Reduce logging overhead\n",
    ")\n",
    "    model.fit(X, y)\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Validation\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__one-hot_encoding.csv\", \"validation_data_drug2__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug1__estate_fingerprints.csv\", \"validation_data_drug2__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + i)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"NCI_test_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y_val['PercentageGrowth']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"Conc1\", \"Conc2\"]]\n",
    "    y_test = y_test[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "    y_test['Conc1'] = np.log2(y_test['Conc1'])\n",
    "    y_test['Conc2'] = np.log2(y_test['Conc2'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "\n",
    "# Define a simple MLP with 4 layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./NCI files/NCI Real Data Fold{fold + 1}/\"\n",
    "    \n",
    "\n",
    "    # Combine one-hot encoded features\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for fname in [\"drug1__one-hot_encoding.csv\", \"drug2__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "    \n",
    "    # Combine auxiliary features\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for fname in [\"drug1__estate_fingerprints.csv\", \"drug2__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "    \n",
    "    # Read training labels and add 'X/X0' as a feature\n",
    "    y_train = pd.read_csv(data_dir + f\"NCI_train_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y_train['PercentageGrowth']\n",
    "    \n",
    "    # Combine all features and remove duplicate columns\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    \n",
    "    # Merge drug concentration labels with features\n",
    "    y = y_train[[\"Conc1\", \"Conc2\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"Conc1\", \"Conc2\"]]\n",
    "    \n",
    "    # Log-transform the labels\n",
    "    y['Conc1'] = np.log2(y['Conc1'])\n",
    "    y['Conc2'] = np.log2(y['Conc2'])\n",
    "    X.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Convert training data to torch tensors\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = MLP(input_dim=input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Free training memory\n",
    "    del X, y, X_tensor, y_tensor\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for fname in [\"validation_data_drug1__one-hot_encoding.csv\", \n",
    "                  \"validation_data_drug2__one-hot_encoding.csv\", \n",
    "                  \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "    \n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for fname in [\"validation_data_drug1__estate_fingerprints.csv\", \n",
    "                  \"validation_data_drug2__estate_fingerprints.csv\", \n",
    "                  \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir + fname)\n",
    "        features_auxiliary = pd.concat([features_auxiliary, temp], axis=1)\n",
    "    \n",
    "    # Read validation labels and add 'X/X0' as a feature\n",
    "    y_val = pd.read_csv(data_dir + f\"NCI_test_fold_{fold}.csv\")\n",
    "    features_tensor['PercentageGrowth'] = y_val['PercentageGrowth']\n",
    "    \n",
    "    # Combine features and remove duplicate columns\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "    # Merge drug concentration labels with validation features\n",
    "    y_test = y_val[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"Conc1\", \"Conc2\"]]\n",
    "    X_test.drop([\"Conc1\", \"Conc2\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Log-transform the validation labels\n",
    "    y_test['Conc1'] = np.log2(y_test['Conc1'])\n",
    "    y_test['Conc2'] = np.log2(y_test['Conc2'])\n",
    "    \n",
    "    # Convert validation data to torch tensors\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy arrays for evaluation\n",
    "    pred_np = predictions.numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_test_np, pred_np)\n",
    "    mae = np.mean(np.abs(y_test_np - pred_np))\n",
    "    print(f\"Fold {fold} - Validation MSE: {mse:.4f}\")\n",
    "    print(f\"Fold {fold} - Validation MAE: {mae:.4f}\")\n",
    "    \n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in predictions:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n",
    "\n",
    "    # Free validation memory and model for the next fold\n",
    "    del X_test, y_test, X_test_tensor, y_test_tensor, model\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Models for AZ-Dream Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug_row__one-hot_encoding.csv\", \"drug_col__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug_row__estate_fingerprints.csv\", \"drug_col__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "     # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"astra_train_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y['inhibition']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    \n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"conc_r\", \"conc_c\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"conc_r\", \"conc_c\"]]\n",
    "    y['conc_r'] = np.log2(y['conc_r'])\n",
    "    y['conc_c'] = np.log2(y['conc_c'])\n",
    "    X.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = ElasticNet()\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "\n",
    "    model.fit(X, y)\n",
    "    features = X.columns\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    seed = 123 # Random seed\n",
    "    #data_dir = \"./our data/final/\"\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold+1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__one-hot_encoding.csv\", \"validation_data_drug_col__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__estate_fingerprints.csv\", \"validation_data_drug_col__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"astra_test_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y_val['inhibition']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"conc_r\", \"conc_c\"]]\n",
    "    y_test = y_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "    X_test = X_test.reindex(columns=features, fill_value=0)\n",
    "\n",
    "    y_test['conc_r'] = np.log2(y_test['conc_r'])\n",
    "    y_test['conc_c'] = np.log2(y_test['conc_c'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug_row__one-hot_encoding.csv\", \"drug_col__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug_row__estate_fingerprints.csv\", \"drug_col__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "     # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"astra_train_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y['inhibition']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    \n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"conc_r\", \"conc_c\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"conc_r\", \"conc_c\"]]\n",
    "    y['conc_r'] = np.log2(y['conc_r'])\n",
    "    y['conc_c'] = np.log2(y['conc_c'])\n",
    "    X.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Start Training for fold {fold}: \")\n",
    "    model = RandomForestRegressor(\n",
    "    n_estimators=50,         # Reduce the number of trees (default is 100)\n",
    "    max_depth=10,            # Limit the depth of trees (default is None)\n",
    "    min_samples_split=10,     # Increase minimum samples per split (default is 2)\n",
    "    min_samples_leaf=4,       # Increase minimum samples per leaf (default is 1)\n",
    "    max_features='sqrt',      # Use a subset of features (default is 'auto')\n",
    "    n_jobs=-1,                # Use all CPU cores\n",
    "    random_state=42,          # Ensure reproducibility\n",
    "    verbose=0                 # Reduce logging overhead\n",
    ")\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "\n",
    "    model.fit(X, y)\n",
    "    features = X.columns\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    seed = 123 # Random seed\n",
    "    #data_dir = \"./our data/final/\"\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold+1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__one-hot_encoding.csv\", \"validation_data_drug_col__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__estate_fingerprints.csv\", \"validation_data_drug_col__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"astra_test_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y_val['inhibition']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"conc_r\", \"conc_c\"]]\n",
    "    y_test = y_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "    X_test = X_test.reindex(columns=features, fill_value=0)\n",
    "\n",
    "    y_test['conc_r'] = np.log2(y_test['conc_r'])\n",
    "    y_test['conc_c'] = np.log2(y_test['conc_c'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict and evaluate\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in pred:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "\n",
    "# Define a simple MLP with 4 layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "for fold in range(5):\n",
    "    import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold + 1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"drug_row__one-hot_encoding.csv\", \"drug_col__one-hot_encoding.csv\", \"cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"drug_row__estate_fingerprints.csv\", \"drug_col__estate_fingerprints.csv\", \"cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "     # Add X/X0 as a feature\n",
    "    y = pd.read_csv(data_dir + f\"astra_train_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y['inhibition']\n",
    "    # Combine all features\n",
    "    X = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "    \n",
    "    # Set drug concentration as the label\n",
    "    y = y[[\"conc_r\", \"conc_c\"]]\n",
    "    X = pd.merge(X, y, left_index=True, right_index=True)\n",
    "    y = X[[\"conc_r\", \"conc_c\"]]\n",
    "    y['conc_r'] = np.log2(y['conc_r'])\n",
    "    y['conc_c'] = np.log2(y['conc_c'])\n",
    "    X.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "    X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
    "    features = X.columns\n",
    "\n",
    "    # Convert training data to torch tensors\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Train the MLP Model\n",
    "    # ---------------------------\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = MLP(input_dim=input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Fold {fold}, Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Free training memory\n",
    "    del X, y, X_tensor, y_tensor\n",
    "    gc.collect()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Prepare Validation Data\n",
    "    # ---------------------------\n",
    "    # Validation phase\n",
    "    data_dir = f\"./Astra files/Astra Real Data Fold{fold+1}/\"\n",
    "\n",
    "    features_tensor = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__one-hot_encoding.csv\", \"validation_data_drug_col__one-hot_encoding.csv\", \"validation_data_cell_lines__one-hot_encoding.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_tensor = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    features_auxiliary = pd.DataFrame()\n",
    "    for i in [\"validation_data_drug_row__estate_fingerprints.csv\", \"validation_data_drug_col__estate_fingerprints.csv\", \"validation_data_cell_lines__gene_expression.csv\"]:\n",
    "        temp = pd.read_csv(data_dir+i)\n",
    "        features_auxiliary = pd.concat([features_tensor, temp], axis=1)\n",
    "\n",
    "    # Add X/X0 as a feature for validation\n",
    "    y_val = pd.read_csv(data_dir + f\"astra_test_fold_{fold}.csv\")\n",
    "    features_tensor['inhibition'] = y_val['inhibition']\n",
    "    # Combine all features for validation\n",
    "    X_test = pd.concat([features_tensor, features_auxiliary], axis=1).dropna()\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "\n",
    "    # Validation label (drug concentrations)\n",
    "    y_test = y_val[[\"conc_r\", \"conc_c\"]]\n",
    "    y_test = y_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    y_test = X_test[[\"conc_r\", \"conc_c\"]]\n",
    "    X_test.drop([\"conc_r\", \"conc_c\"], axis=1, inplace=True)\n",
    "    X_test = X_test.reindex(columns=features, fill_value=0)\n",
    "\n",
    "    y_test['conc_r'] = np.log2(y_test['conc_r'])\n",
    "    y_test['conc_c'] = np.log2(y_test['conc_c'])\n",
    "    \n",
    "    # Convert validation data to torch tensors\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Validate the Model\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy arrays for evaluation\n",
    "    pred_np = predictions.numpy()\n",
    "    y_test_np = y_test_tensor.numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_test_np, pred_np)\n",
    "    mae = np.mean(np.abs(y_test_np - pred_np))\n",
    "    print(f\"Fold {fold} - Validation MSE: {mse:.4f}\")\n",
    "    print(f\"Fold {fold} - Validation MAE: {mae:.4f}\")\n",
    "    \n",
    "    p1 = list()\n",
    "    p2 = list()\n",
    "    for i in predictions:\n",
    "        p1.append(complex(i[0], i[1]))\n",
    "    for i in y_test.to_numpy():\n",
    "        p2.append(complex(i[0], i[1]))\n",
    "    mse = np.mean(pow(abs(np.array(p1)-np.array(p2)), 2))\n",
    "    mae = np.mean(abs(np.array(p1)-np.array(p2)))\n",
    "    print(f\"Mean squared error for fold {fold}: {mse}\")\n",
    "    print(f\"Mean squared error for fold {fold}: {mae}\")\n",
    "\n",
    "    # Free validation memory and model for the next fold\n",
    "    del X_test, y_test, X_test_tensor, y_test_tensor, model\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
